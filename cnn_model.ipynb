import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,
                             roc_curve, auc, precision_score, recall_score,
                             fbeta_score, cohen_kappa_score)
from sklearn.preprocessing import label_binarize
from imblearn.over_sampling import SMOTE
import warnings
warnings.filterwarnings('ignore')

train_data = pd.read_csv("train.csv").dropna()
print(f"Total samples: {train_data.shape[0]}")
print(f"Classes distribution:\n{train_data['score'].value_counts()}")


max_features = 20000  
max_len = 500        

tokenizer = Tokenizer(num_words=max_features, oov_token="<OOV>")
tokenizer.fit_on_texts(train_data['full_text'])
X_sequences = tokenizer.texts_to_sequences(train_data['full_text'])

X_padded = pad_sequences(X_sequences, maxlen=max_len, padding='post', truncating='post')

y = train_data['score'].astype(int).values
y = y - y.min()
num_classes = len(np.unique(y))
print(f"Number of classes: {num_classes}")

X_train, X_temp, y_train, y_temp = train_test_split(
    X_padded, y, test_size=0.2, random_state=42, stratify=y
)
X_eval, X_test, y_eval, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

print(f"Training samples: {X_train.shape[0]}")
print(f"Validation samples: {X_eval.shape[0]}")
print(f"Test samples: {X_test.shape[0]}")

embedding_dim = 128

model = Sequential([
    Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=max_len),
    Conv1D(filters=128, kernel_size=5, activation='relu'),
    GlobalMaxPooling1D(),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history = model.fit(
    X_train, y_train,
    epochs=20,
    batch_size=128,
    validation_data=(X_eval, y_eval),
    callbacks=[early_stop],
    verbose=1
)

y_pred_test_probs = model.predict(X_test)
y_pred_test = np.argmax(y_pred_test_probs, axis=1)

metrics = {}
for dataset, y_true, y_pred in [
    ('Train', y_train, np.argmax(model.predict(X_train), axis=1)),
    ('Eval', y_eval, np.argmax(model.predict(X_eval), axis=1)),
    ('Test', y_test, y_pred_test)
]:
    metrics[f'{dataset} Accuracy'] = accuracy_score(y_true, y_pred)
    metrics[f'{dataset} Precision (Macro)'] = precision_score(y_true, y_pred, average='macro', zero_division=0)
    metrics[f'{dataset} Recall (Macro)'] = recall_score(y_true, y_pred, average='macro', zero_division=0)
    metrics[f'{dataset} F1 Score (Macro)'] = fbeta_score(y_true, y_pred, beta=1, average='macro', zero_division=0)
    metrics[f'{dataset} QWK'] = cohen_kappa_score(y_true, y_pred, weights='quadratic')

metrics['Model'] = 'CNN'
performance_summary = pd.DataFrame([metrics]).set_index('Model')

print("Classification Report on Test Set:\n")
print(classification_report(y_test, y_pred_test, zero_division=0))

cm = confusion_matrix(y_test, y_pred_test)
plt.figure(figsize=(10,8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.title('Confusion Matrix for CNN')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

y_test_binarized = label_binarize(y_test, classes=np.unique(y))
y_score = y_pred_test_probs

fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(num_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

plt.figure(figsize=(10,8))
colors = sns.color_palette("hsv", num_classes)
for i in range(num_classes):
    plt.plot(fpr[i], tpr[i], color=colors[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')
plt.plot([0,1], [0,1], 'k--', label='Random Classifier (AUC = 0.50)')
plt.title('ROC Curves for CNN')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()

report = "## Model Performance Summary\n\n"
report += performance_summary.to_markdown() + "\n\n"
report += "## Detailed Metrics\n\n"
report += f"### CNN Model\n\n"
report += classification_report(y_test, y_pred_test, zero_division=0) + "\n\n"
report += f"**Model Architecture:**\n```\n{model.summary()}\n```\n\n"
report += f"**Training History:**\n"
history_df = pd.DataFrame(history.history)
report += history_df.to_markdown() + "\n\n"

with open("model_performance_report_cnn.md", "w") as file:
    file.write(report)

print(f"Test Accuracy: {metrics['Test Accuracy']:.4f}")
print("Performance report generated and saved as 'model_performance_report_cnn.md'.")

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()
